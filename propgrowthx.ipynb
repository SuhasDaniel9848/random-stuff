{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "import pandas \n",
    "dataframe = pandas.read_csv('NY-House-Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILE PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\suhas\\OneDrive\\Desktop\\PROPGROWTHX  AI PROJECT TRIAL\n",
      "\n",
      "Files in current directory:\n",
      "  - from openai import OpenAI.py\n",
      "  - NY-House-Dataset-Modified.csv\n",
      "  - NY-House-Dataset.csv\n",
      "  - NYC_House_Dataset-Cleaned.csv\n",
      "  - nyc_property_map.html\n",
      "  - propgrowthx.ipynb\n",
      "\n",
      "CSV files found: ['NY-House-Dataset-Modified.csv', 'NY-House-Dataset.csv', 'NYC_House_Dataset-Cleaned.csv']\n",
      "\n",
      "Loading NYC_House_Dataset-Cleaned.csv...\n",
      "\n",
      "Data preview:\n",
      "                                     BROKERTITLE            TYPE   PRICE  \\\n",
      "0    Brokered by Douglas Elliman  -111 Fifth Ave  Condo for sale  315000   \n",
      "1                         Brokered by Sowae Corp  House for sale  260000   \n",
      "2                            Brokered by COMPASS  Condo for sale   69000   \n",
      "3                         Brokered by Sowae Corp  House for sale  690000   \n",
      "4  Brokered by Douglas Elliman - 575 Madison Ave  Condo for sale  899500   \n",
      "\n",
      "   BEDS  BATH  PROPERTYSQFT                  ADDRESS                    STATE  \\\n",
      "0     2   2.0   1400.000000     2 E 55th St Unit 803       New York, NY 10022   \n",
      "1     4   2.0   2015.000000         620 Sinclair Ave  Staten Island, NY 10312   \n",
      "2     3   1.0    445.000000  2 E 55th St Unit 908W33      Manhattan, NY 10022   \n",
      "3     5   2.0   4004.000000              584 Park Pl       Brooklyn, NY 11238   \n",
      "4     2   2.0   2184.207862   157 W 126th St Unit 1B       New York, NY 10027   \n",
      "\n",
      "                                 MAIN_ADDRESS ADMINISTRATIVE_AREA_LEVEL_2  \\\n",
      "0      2 E 55th St Unit 803New York, NY 10022             New York County   \n",
      "1     620 Sinclair AveStaten Island, NY 10312               United States   \n",
      "2  2 E 55th St Unit 908W33Manhattan, NY 10022               United States   \n",
      "3               584 Park PlBrooklyn, NY 11238               United States   \n",
      "4    157 W 126th St Unit 1BNew York, NY 10027                    New York   \n",
      "\n",
      "          LOCALITY      SUBLOCALITY       STREET_NAME         LONG_NAME  \\\n",
      "0         New York        Manhattan  East 55th Street   Regis Residence   \n",
      "1         New York  Richmond County     Staten Island   Sinclair Avenue   \n",
      "2         New York  New York County          New York  East 55th Street   \n",
      "3         New York     Kings County          Brooklyn        Park Place   \n",
      "4  New York County         New York         Manhattan               157   \n",
      "\n",
      "                                   FORMATTED_ADDRESS   LATITUDE  LONGITUDE  \n",
      "0  Regis Residence, 2 E 55th St #803, New York, N...  40.761255 -73.974483  \n",
      "1     620 Sinclair Ave, Staten Island, NY 10312, USA  40.541805 -74.196109  \n",
      "2               2 E 55th St, New York, NY 10022, USA  40.761398 -73.974613  \n",
      "3               584 Park Pl, Brooklyn, NY 11238, USA  40.674363 -73.958725  \n",
      "4        157 W 126th St #1b, New York, NY 10027, USA  40.809448 -73.946777  \n",
      "\n",
      "Columns:\n",
      "Index(['BROKERTITLE', 'TYPE', 'PRICE', 'BEDS', 'BATH', 'PROPERTYSQFT',\n",
      "       'ADDRESS', 'STATE', 'MAIN_ADDRESS', 'ADMINISTRATIVE_AREA_LEVEL_2',\n",
      "       'LOCALITY', 'SUBLOCALITY', 'STREET_NAME', 'LONG_NAME',\n",
      "       'FORMATTED_ADDRESS', 'LATITUDE', 'LONGITUDE'],\n",
      "      dtype='object')\n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4552 entries, 0 to 4551\n",
      "Data columns (total 17 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   BROKERTITLE                  4552 non-null   object \n",
      " 1   TYPE                         4552 non-null   object \n",
      " 2   PRICE                        4552 non-null   int64  \n",
      " 3   BEDS                         4552 non-null   int64  \n",
      " 4   BATH                         4552 non-null   float64\n",
      " 5   PROPERTYSQFT                 4552 non-null   float64\n",
      " 6   ADDRESS                      4552 non-null   object \n",
      " 7   STATE                        4552 non-null   object \n",
      " 8   MAIN_ADDRESS                 4552 non-null   object \n",
      " 9   ADMINISTRATIVE_AREA_LEVEL_2  4552 non-null   object \n",
      " 10  LOCALITY                     4552 non-null   object \n",
      " 11  SUBLOCALITY                  4552 non-null   object \n",
      " 12  STREET_NAME                  4552 non-null   object \n",
      " 13  LONG_NAME                    4552 non-null   object \n",
      " 14  FORMATTED_ADDRESS            4552 non-null   object \n",
      " 15  LATITUDE                     4552 non-null   float64\n",
      " 16  LONGITUDE                    4552 non-null   float64\n",
      "dtypes: float64(4), int64(2), object(11)\n",
      "memory usage: 604.7+ KB\n",
      "None\n",
      "\n",
      "Preparing data for training...\n",
      "Missing values in each column:\n",
      "BROKERTITLE                    0\n",
      "TYPE                           0\n",
      "PRICE                          0\n",
      "BEDS                           0\n",
      "BATH                           0\n",
      "PROPERTYSQFT                   0\n",
      "ADDRESS                        0\n",
      "STATE                          0\n",
      "MAIN_ADDRESS                   0\n",
      "ADMINISTRATIVE_AREA_LEVEL_2    0\n",
      "LOCALITY                       0\n",
      "SUBLOCALITY                    0\n",
      "STREET_NAME                    0\n",
      "LONG_NAME                      0\n",
      "FORMATTED_ADDRESS              0\n",
      "LATITUDE                       0\n",
      "LONGITUDE                      0\n",
      "dtype: int64\n",
      "\n",
      "Rows before dropping nulls: 4552\n",
      "Rows after dropping nulls: 4552\n",
      "\n",
      "Data is now ready for training models.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory first\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# List files in current directory to see what's available\n",
    "print(\"\\nFiles in current directory:\")\n",
    "for file in os.listdir(current_dir):\n",
    "    print(f\"  - {file}\")\n",
    "    \n",
    "# Check if our target CSV files exist in the current directory\n",
    "csv_files = [f for f in os.listdir(current_dir) if f.endswith('.csv')]\n",
    "print(f\"\\nCSV files found: {csv_files}\")\n",
    "\n",
    "# Try loading the NYC_House_Dataset-Cleaned.csv file if it exists\n",
    "target_file = \"NYC_House_Dataset-Cleaned.csv\"\n",
    "if target_file in csv_files:\n",
    "    print(f\"\\nLoading {target_file}...\")\n",
    "    dataframe_cleaned = pd.read_csv(target_file)\n",
    "    \n",
    "    # Verify the data\n",
    "    print(\"\\nData preview:\")\n",
    "    print(dataframe_cleaned.head())\n",
    "    print(\"\\nColumns:\")\n",
    "    print(dataframe_cleaned.columns)\n",
    "    print(\"\\nData info:\")\n",
    "    print(dataframe_cleaned.info())\n",
    "    \n",
    "    # Additional step: prepare data for training\n",
    "    print(\"\\nPreparing data for training...\")\n",
    "    \n",
    "    # Basic preprocessing steps for demonstration\n",
    "    # 1. Check for missing values\n",
    "    print(\"Missing values in each column:\")\n",
    "    print(dataframe_cleaned.isnull().sum())\n",
    "    \n",
    "    # 2. Drop rows with missing values or fill them\n",
    "    dataframe_cleaned_no_nulls = dataframe_cleaned.dropna()\n",
    "    print(f\"\\nRows before dropping nulls: {len(dataframe_cleaned)}\")\n",
    "    print(f\"Rows after dropping nulls: {len(dataframe_cleaned_no_nulls)}\")\n",
    "    \n",
    "    # Ready for training\n",
    "    print(\"\\nData is now ready for training models.\")\n",
    "else:\n",
    "    # If the specific file is not found, try to load any available CSV\n",
    "    if csv_files:\n",
    "        print(f\"\\nTarget file '{target_file}' not found. Attempting to load the first available CSV: {csv_files[0]}\")\n",
    "        dataframe_cleaned = pd.read_csv(csv_files[0])\n",
    "        \n",
    "        # Display the data\n",
    "        print(\"\\nData preview:\")\n",
    "        print(dataframe_cleaned.head())\n",
    "    else:\n",
    "        print(\"\\nNo CSV files found in the current directory.\")\n",
    "        print(\"Please check that your dataset is in the current working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: c:\\Users\\suhas\\OneDrive\\Desktop\\PROPGROWTHX  AI PROJECT TRIAL\n",
      "CSV file loaded successfully!\n",
      "\n",
      "ðŸ“Œ Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4801 entries, 0 to 4800\n",
      "Data columns (total 17 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   BROKERTITLE                  4801 non-null   object \n",
      " 1   TYPE                         4801 non-null   object \n",
      " 2   PRICE                        4801 non-null   int64  \n",
      " 3   BEDS                         4801 non-null   int64  \n",
      " 4   BATH                         4801 non-null   float64\n",
      " 5   PROPERTYSQFT                 4801 non-null   float64\n",
      " 6   ADDRESS                      4801 non-null   object \n",
      " 7   STATE                        4801 non-null   object \n",
      " 8   MAIN_ADDRESS                 4801 non-null   object \n",
      " 9   ADMINISTRATIVE_AREA_LEVEL_2  4801 non-null   object \n",
      " 10  LOCALITY                     4801 non-null   object \n",
      " 11  SUBLOCALITY                  4801 non-null   object \n",
      " 12  STREET_NAME                  4801 non-null   object \n",
      " 13  LONG_NAME                    4801 non-null   object \n",
      " 14  FORMATTED_ADDRESS            4801 non-null   object \n",
      " 15  LATITUDE                     4801 non-null   float64\n",
      " 16  LONGITUDE                    4801 non-null   float64\n",
      "dtypes: float64(4), int64(2), object(11)\n",
      "memory usage: 637.8+ KB\n",
      "None\n",
      "\n",
      "ðŸ“Œ First 5 rows of the dataset:\n",
      "                                         BROKERTITLE                TYPE  \\\n",
      "0        Brokered by Douglas Elliman  -111 Fifth Ave      Condo for sale   \n",
      "1                                Brokered by Serhant      Condo for sale   \n",
      "2                             Brokered by Sowae Corp      House for sale   \n",
      "3                                Brokered by COMPASS      Condo for sale   \n",
      "4  Brokered by Sotheby's International Realty - E...  Townhouse for sale   \n",
      "\n",
      "       PRICE  BEDS       BATH  PROPERTYSQFT  \\\n",
      "0     315000     2   2.000000        1400.0   \n",
      "1  195000000     7  10.000000       17545.0   \n",
      "2     260000     4   2.000000        2015.0   \n",
      "3      69000     3   1.000000         445.0   \n",
      "4   55000000     7   2.373861       14175.0   \n",
      "\n",
      "                                             ADDRESS                    STATE  \\\n",
      "0                               2 E 55th St Unit 803       New York, NY 10022   \n",
      "1  Central Park Tower Penthouse-217 W 57th New Yo...       New York, NY 10019   \n",
      "2                                   620 Sinclair Ave  Staten Island, NY 10312   \n",
      "3                            2 E 55th St Unit 908W33      Manhattan, NY 10022   \n",
      "4                                        5 E 64th St       New York, NY 10065   \n",
      "\n",
      "                                        MAIN_ADDRESS  \\\n",
      "0             2 E 55th St Unit 803New York, NY 10022   \n",
      "1  Central Park Tower Penthouse-217 W 57th New Yo...   \n",
      "2            620 Sinclair AveStaten Island, NY 10312   \n",
      "3         2 E 55th St Unit 908W33Manhattan, NY 10022   \n",
      "4                      5 E 64th StNew York, NY 10065   \n",
      "\n",
      "  ADMINISTRATIVE_AREA_LEVEL_2  LOCALITY      SUBLOCALITY       STREET_NAME  \\\n",
      "0             New York County  New York        Manhattan  East 55th Street   \n",
      "1               United States  New York  New York County          New York   \n",
      "2               United States  New York  Richmond County     Staten Island   \n",
      "3               United States  New York  New York County          New York   \n",
      "4               United States  New York  New York County          New York   \n",
      "\n",
      "          LONG_NAME                                  FORMATTED_ADDRESS  \\\n",
      "0   Regis Residence  Regis Residence, 2 E 55th St #803, New York, N...   \n",
      "1  West 57th Street             217 W 57th St, New York, NY 10019, USA   \n",
      "2   Sinclair Avenue     620 Sinclair Ave, Staten Island, NY 10312, USA   \n",
      "3  East 55th Street               2 E 55th St, New York, NY 10022, USA   \n",
      "4  East 64th Street               5 E 64th St, New York, NY 10065, USA   \n",
      "\n",
      "    LATITUDE  LONGITUDE  \n",
      "0  40.761255 -73.974483  \n",
      "1  40.766393 -73.980991  \n",
      "2  40.541805 -74.196109  \n",
      "3  40.761398 -73.974613  \n",
      "4  40.767224 -73.969856  \n",
      "\n",
      "ðŸ“Œ Dataset Shape (Rows, Columns): (4801, 17)\n",
      "\n",
      "ðŸ“Œ Column Names: ['BROKERTITLE', 'TYPE', 'PRICE', 'BEDS', 'BATH', 'PROPERTYSQFT', 'ADDRESS', 'STATE', 'MAIN_ADDRESS', 'ADMINISTRATIVE_AREA_LEVEL_2', 'LOCALITY', 'SUBLOCALITY', 'STREET_NAME', 'LONG_NAME', 'FORMATTED_ADDRESS', 'LATITUDE', 'LONGITUDE']\n",
      "\n",
      "ðŸ“Œ Missing Values in Each Column:\n",
      "BROKERTITLE                    0\n",
      "TYPE                           0\n",
      "PRICE                          0\n",
      "BEDS                           0\n",
      "BATH                           0\n",
      "PROPERTYSQFT                   0\n",
      "ADDRESS                        0\n",
      "STATE                          0\n",
      "MAIN_ADDRESS                   0\n",
      "ADMINISTRATIVE_AREA_LEVEL_2    0\n",
      "LOCALITY                       0\n",
      "SUBLOCALITY                    0\n",
      "STREET_NAME                    0\n",
      "LONG_NAME                      0\n",
      "FORMATTED_ADDRESS              0\n",
      "LATITUDE                       0\n",
      "LONGITUDE                      0\n",
      "dtype: int64\n",
      "\n",
      "ðŸ“Œ Statistical Summary of Numeric Columns:\n",
      "              PRICE         BEDS         BATH  PROPERTYSQFT     LATITUDE  \\\n",
      "count  4.801000e+03  4801.000000  4801.000000   4801.000000  4801.000000   \n",
      "mean   2.356940e+06     3.356801     2.373861   2184.207862    40.714227   \n",
      "std    3.135525e+07     2.602315     1.946962   2377.140894     0.087676   \n",
      "min    2.494000e+03     1.000000     0.000000    230.000000    40.499546   \n",
      "25%    4.990000e+05     2.000000     1.000000   1200.000000    40.639375   \n",
      "50%    8.250000e+05     3.000000     2.000000   2184.207862    40.726749   \n",
      "75%    1.495000e+06     4.000000     3.000000   2184.207862    40.771923   \n",
      "max    2.147484e+09    50.000000    50.000000  65535.000000    40.912729   \n",
      "\n",
      "         LONGITUDE  \n",
      "count  4801.000000  \n",
      "mean    -73.941601  \n",
      "std       0.101082  \n",
      "min     -74.253033  \n",
      "25%     -73.987143  \n",
      "50%     -73.949189  \n",
      "75%     -73.870638  \n",
      "max     -73.702450  \n",
      "\n",
      "âœ… Modified CSV saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the current working directory (to check where the file should be)\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    dataframe = pd.read_csv('NY-House-Dataset.csv')\n",
    "    print(\"CSV file loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found! Make sure 'NY-House-Dataset.csv' is in the correct folder.\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"\\nðŸ“Œ Dataset Info:\")\n",
    "print(dataframe.info())\n",
    "\n",
    "# Show first 5 rows\n",
    "print(\"\\nðŸ“Œ First 5 rows of the dataset:\")\n",
    "print(dataframe.head())\n",
    "\n",
    "# Show the number of rows and columns\n",
    "print(\"\\nðŸ“Œ Dataset Shape (Rows, Columns):\", dataframe.shape)\n",
    "\n",
    "# Show column names\n",
    "print(\"\\nðŸ“Œ Column Names:\", dataframe.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nðŸ“Œ Missing Values in Each Column:\")\n",
    "print(dataframe.isnull().sum())\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\nðŸ“Œ Statistical Summary of Numeric Columns:\")\n",
    "print(dataframe.describe())\n",
    "\n",
    "# Save the dataset after modifications (if needed)\n",
    "dataframe.to_csv('NY-House-Dataset-Modified.csv', index=False)\n",
    "print(\"\\nâœ… Modified CSV saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEAN DATASET AFTER FILTRATION (10,000$ TO $10MILLLION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing outliers: (4552, 17)\n",
      "Cleaned CSV saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Filter out extreme outliers\n",
    "# Keep properties with price between $10,000 and $10M, beds/baths <= 10, and reasonable square footage\n",
    "dataframe_cleaned = dataframe[\n",
    "    (dataframe['PRICE'].between(10000, 10000000)) &\n",
    "    (dataframe['BEDS'] <= 10) &\n",
    "    (dataframe['BATH'] <= 10) &\n",
    "    (dataframe['PROPERTYSQFT'] <= 10000)\n",
    "]\n",
    "\n",
    "# Check the new shape\n",
    "print(\"Shape after removing outliers:\", dataframe_cleaned.shape)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "dataframe_cleaned.to_csv('NYC_House_Dataset-Cleaned.csv', index=False)\n",
    "print(\"Cleaned CSV saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: $1,168,453.87\n",
      "Predicted price for a 2000 sq ft, 3 bed, 2 bath property: $1,210,465.51\n",
      "\n",
      "Undervalued Properties (Predicted Price - Actual Price > $100,000):\n",
      "                   ADDRESS   PRICE  PREDICTED_PRICE  PRICE_DIFFERENCE\n",
      "0     2 E 55th St Unit 803  315000     1.061922e+06      7.469218e+05\n",
      "2         620 Sinclair Ave  260000     1.071481e+06      8.114809e+05\n",
      "5              584 Park Pl  690000     1.902720e+06      1.212720e+06\n",
      "6   157 W 126th St Unit 1B  899500     1.447361e+06      5.478611e+05\n",
      "8  875 Morrison Ave Apt 3M  265000     3.840933e+05      1.190933e+05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\suhas\\AppData\\Local\\Temp\\ipykernel_22444\\716758546.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_cleaned['PREDICTED_PRICE'] = model.predict(X)\n",
      "C:\\Users\\suhas\\AppData\\Local\\Temp\\ipykernel_22444\\716758546.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe_cleaned['PRICE_DIFFERENCE'] = dataframe_cleaned['PREDICTED_PRICE'] - dataframe_cleaned['PRICE']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features (independent variables) and target (dependent variable)\n",
    "X = dataframe_cleaned[['PROPERTYSQFT', 'BEDS', 'BATH']]  # Features\n",
    "y = dataframe_cleaned['PRICE']  # Target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error: ${rmse:,.2f}\")\n",
    "\n",
    "# Example: Predict the price of a property with 2000 sq ft, 3 beds, 2 baths\n",
    "example_property = np.array([[2000, 3, 2]])\n",
    "predicted_price = model.predict(example_property)\n",
    "print(f\"Predicted price for a 2000 sq ft, 3 bed, 2 bath property: ${predicted_price[0]:,.2f}\")\n",
    "\n",
    "# Add predicted prices to the dataset\n",
    "dataframe_cleaned['PREDICTED_PRICE'] = model.predict(X)\n",
    "dataframe_cleaned['PRICE_DIFFERENCE'] = dataframe_cleaned['PREDICTED_PRICE'] - dataframe_cleaned['PRICE']\n",
    "\n",
    "# Identify undervalued properties (where actual price is much lower than predicted)\n",
    "undervalued = dataframe_cleaned[dataframe_cleaned['PRICE_DIFFERENCE'] > 100000]\n",
    "print(\"\\nUndervalued Properties (Predicted Price - Actual Price > $100,000):\")\n",
    "print(undervalued[['ADDRESS', 'PRICE', 'PREDICTED_PRICE', 'PRICE_DIFFERENCE']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: $1,168,453.87\n",
      "Predicted price for a 2000 sq ft, 3 bed, 2 bath property: $1,210,465.51\n",
      "\n",
      "Undervalued Properties (Predicted Price - Actual Price > $100,000):\n",
      "                   ADDRESS   PRICE  PREDICTED_PRICE  PRICE_DIFFERENCE\n",
      "0     2 E 55th St Unit 803  315000     1.061922e+06      7.469218e+05\n",
      "1         620 Sinclair Ave  260000     1.071481e+06      8.114809e+05\n",
      "3              584 Park Pl  690000     1.902720e+06      1.212720e+06\n",
      "4   157 W 126th St Unit 1B  899500     1.447361e+06      5.478611e+05\n",
      "5  875 Morrison Ave Apt 3M  265000     3.840933e+05      1.190933e+05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the cleaned dataset (if you saved it earlier)\n",
    "dataframe = pd.read_csv('NYC_House_Dataset-Cleaned.csv')\n",
    "\n",
    "# If you haven't saved it yet, re-run the cleaning step with .copy()\n",
    "dataframe_cleaned = dataframe[\n",
    "    (dataframe['PRICE'].between(10000, 10000000)) &\n",
    "    (dataframe['BEDS'] <= 10) &\n",
    "    (dataframe['BATH'] <= 10) &\n",
    "    (dataframe['PROPERTYSQFT'] <= 10000)\n",
    "].copy()  # Add .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Features and target\n",
    "X = dataframe_cleaned[['PROPERTYSQFT', 'BEDS', 'BATH']]\n",
    "y = dataframe_cleaned['PRICE']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error: ${rmse:,.2f}\")\n",
    "\n",
    "# Predict for a sample property (fix the feature names warning)\n",
    "example_property = pd.DataFrame([[2000, 3, 2]], columns=['PROPERTYSQFT', 'BEDS', 'BATH'])\n",
    "predicted_price = model.predict(example_property)\n",
    "print(f\"Predicted price for a 2000 sq ft, 3 bed, 2 bath property: ${predicted_price[0]:,.2f}\")\n",
    "\n",
    "# Add predicted prices to the dataset\n",
    "dataframe_cleaned['PREDICTED_PRICE'] = model.predict(X)\n",
    "dataframe_cleaned['PRICE_DIFFERENCE'] = dataframe_cleaned['PREDICTED_PRICE'] - dataframe_cleaned['PRICE']\n",
    "\n",
    "# Identify undervalued properties\n",
    "undervalued = dataframe_cleaned[dataframe_cleaned['PRICE_DIFFERENCE'] > 100000]\n",
    "print(\"\\nUndervalued Properties (Predicted Price - Actual Price > $100,000):\")\n",
    "print(undervalued[['ADDRESS', 'PRICE', 'PREDICTED_PRICE', 'PRICE_DIFFERENCE']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE: $1,138,544.09\n",
      "Random Forest Predicted price for a 2000 sq ft, 3 bed, 2 bath property: $995,836.63\n",
      "\n",
      "Undervalued Properties (Random Forest, Predicted Price - Actual Price > $100,000):\n",
      "                  ADDRESS   PRICE  PREDICTED_PRICE_RF  PRICE_DIFFERENCE_RF\n",
      "0    2 E 55th St Unit 803  315000        8.653188e+05         5.503188e+05\n",
      "1        620 Sinclair Ave  260000        4.964949e+05         2.364949e+05\n",
      "3             584 Park Pl  690000        1.894275e+06         1.204275e+06\n",
      "4  157 W 126th St Unit 1B  899500        1.447413e+06         5.479133e+05\n",
      "9   34-41 85th St Unit 1D  259000        4.478412e+05         1.888412e+05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print(f\"Random Forest RMSE: ${rmse_rf:,.2f}\")\n",
    "\n",
    "# Predict for the sample property\n",
    "predicted_price_rf = rf_model.predict(example_property)\n",
    "print(f\"Random Forest Predicted price for a 2000 sq ft, 3 bed, 2 bath property: ${predicted_price_rf[0]:,.2f}\")\n",
    "\n",
    "# Update predicted prices with Random Forest\n",
    "dataframe_cleaned['PREDICTED_PRICE_RF'] = rf_model.predict(X)\n",
    "dataframe_cleaned['PRICE_DIFFERENCE_RF'] = dataframe_cleaned['PREDICTED_PRICE_RF'] - dataframe_cleaned['PRICE']\n",
    "\n",
    "# Identify undervalued properties with Random Forest\n",
    "undervalued_rf = dataframe_cleaned[dataframe_cleaned['PRICE_DIFFERENCE_RF'] > 100000]\n",
    "print(\"\\nUndervalued Properties (Random Forest, Predicted Price - Actual Price > $100,000):\")\n",
    "print(undervalued_rf[['ADDRESS', 'PRICE', 'PREDICTED_PRICE_RF', 'PRICE_DIFFERENCE_RF']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive map saved as 'nyc_property_map.html'. Open it in a browser to view!\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Create a map centered on NYC\n",
    "m = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# Add markers for each property\n",
    "for idx, row in dataframe_cleaned.iterrows():\n",
    "    color = 'green' if row['PRICE_DIFFERENCE_RF'] > 100000 else 'blue'\n",
    "    folium.Marker(\n",
    "        location=[row['LATITUDE'], row['LONGITUDE']],\n",
    "        popup=f\"{row['ADDRESS']}<br>Price: ${row['PRICE']:,.0f}<br>Predicted: ${row['PREDICTED_PRICE_RF']:,.0f}\",\n",
    "        icon=folium.Icon(color=color)\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map\n",
    "m.save('nyc_property_map.html')\n",
    "print(\"Interactive map saved as 'nyc_property_map.html'. Open it in a browser to view!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in c:\\users\\suhas\\anaconda3\\lib\\site-packages (0.19.5)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from folium) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from folium) (2022.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from requests->folium) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from requests->folium) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from requests->folium) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from requests->folium) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING USING GPU RTX 4060 FOR BETTER RESULTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m lgb_train \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_train, y_train)\n\u001b[0;32m     32\u001b[0m lgb_test \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_test, y_test, reference\u001b[38;5;241m=\u001b[39mlgb_train)\n\u001b[1;32m---> 33\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(lgb_params, lgb_train, valid_sets\u001b[38;5;241m=\u001b[39m[lgb_test])\n\u001b[0;32m     35\u001b[0m y_pred_lgb \u001b[38;5;241m=\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     36\u001b[0m rmse_lgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_test, y_pred_lgb))\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m Booster(params\u001b[38;5;241m=\u001b[39mparams, train_set\u001b[38;5;241m=\u001b[39mtrain_set)\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:3660\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n\u001b[0;32m   3659\u001b[0m params_str \u001b[38;5;241m=\u001b[39m _param_dict_to_str(params)\n\u001b[1;32m-> 3660\u001b[0m _safe_call(\n\u001b[0;32m   3661\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterCreate(\n\u001b[0;32m   3662\u001b[0m         train_set\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   3663\u001b[0m         _c_str(params_str),\n\u001b[0;32m   3664\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle),\n\u001b[0;32m   3665\u001b[0m     )\n\u001b[0;32m   3666\u001b[0m )\n\u001b[0;32m   3667\u001b[0m \u001b[38;5;66;03m# save reference to data\u001b[39;00m\n\u001b[0;32m   3668\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set \u001b[38;5;241m=\u001b[39m train_set\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:313\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Features and target\n",
    "# Encode categorical variables (SUBLOCALITY) as dummy variables\n",
    "X = pd.get_dummies(dataframe_cleaned[['PROPERTYSQFT', 'BEDS', 'BATH', 'SUBLOCALITY']], drop_first=True)\n",
    "y = dataframe_cleaned['PRICE']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. LightGBM with GPU support\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'device': 'gpu',  # Use GPU (RTX 4060)\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_test])\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\n",
    "print(f\"LightGBM RMSE: ${rmse_lgb:,.2f}\")\n",
    "\n",
    "# 2. XGBoost with GPU support\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',  # Use GPU\n",
    "    'gpu_id': 0,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 6,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "print(f\"XGBoost RMSE: ${rmse_xgb:,.2f}\")\n",
    "\n",
    "# 3. CatBoost (CPU, as GPU support is less straightforward)\n",
    "cb_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# CatBoost can handle categorical features directly, so we donâ€™t need dummy variables\n",
    "X_cat = dataframe_cleaned[['PROPERTYSQFT', 'BEDS', 'BATH', 'SUBLOCALITY']]\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y, test_size=0.2, random_state=42)\n",
    "\n",
    "cb_model = cb.CatBoostRegressor(**cb_params)\n",
    "cb_model.fit(X_train_cat, y_train_cat, eval_set=(X_test_cat, y_test_cat), cat_features=['SUBLOCALITY'])\n",
    "\n",
    "y_pred_cb = cb_model.predict(X_test_cat)\n",
    "rmse_cb = np.sqrt(mean_squared_error(y_test_cat, y_pred_cb))\n",
    "print(f\"CatBoost RMSE: ${rmse_cb:,.2f}\")\n",
    "\n",
    "# Choose the best model (lowest RMSE)\n",
    "best_model = min([\n",
    "    ('LightGBM', lgb_model, rmse_lgb),\n",
    "    ('XGBoost', xgb_model, rmse_xgb),\n",
    "    ('CatBoost', cb_model, rmse_cb)\n",
    "], key=lambda x: x[2])\n",
    "\n",
    "print(f\"\\nBest Model: {best_model[0]} with RMSE: ${best_model[2]:,.2f}\")\n",
    "\n",
    "# Use the best model to predict prices for the entire dataset\n",
    "if best_model[0] == 'LightGBM':\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = lgb_model.predict(X)\n",
    "elif best_model[0] == 'XGBoost':\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = xgb_model.predict(X)\n",
    "else:\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = cb_model.predict(X_cat)\n",
    "\n",
    "dataframe_cleaned['PRICE_DIFFERENCE'] = dataframe_cleaned['PREDICTED_PRICE'] - dataframe_cleaned['PRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIGHTGBM VERSION INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6.0\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "print(lgb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE AMONG BIG THREE (XGBOOST, CATBOOST, LIGHTBGM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\suhas\\OneDrive\\Desktop\\PROPGROWTHX  AI PROJECT TRIAL\n",
      "CSV files found: ['NY-House-Dataset-Modified.csv', 'NY-House-Dataset.csv', 'NYC_House_Dataset-Cleaned.csv']\n",
      "Loading NYC_House_Dataset-Cleaned.csv...\n",
      "\n",
      "Dataset Info:\n",
      "Shape: (4552, 17)\n",
      "\n",
      "Columns:\n",
      "['BROKERTITLE', 'TYPE', 'PRICE', 'BEDS', 'BATH', 'PROPERTYSQFT', 'ADDRESS', 'STATE', 'MAIN_ADDRESS', 'ADMINISTRATIVE_AREA_LEVEL_2', 'LOCALITY', 'SUBLOCALITY', 'STREET_NAME', 'LONG_NAME', 'FORMATTED_ADDRESS', 'LATITUDE', 'LONGITUDE']\n",
      "\n",
      "First few rows:\n",
      "                                     BROKERTITLE            TYPE   PRICE  \\\n",
      "0    Brokered by Douglas Elliman  -111 Fifth Ave  Condo for sale  315000   \n",
      "1                         Brokered by Sowae Corp  House for sale  260000   \n",
      "2                            Brokered by COMPASS  Condo for sale   69000   \n",
      "3                         Brokered by Sowae Corp  House for sale  690000   \n",
      "4  Brokered by Douglas Elliman - 575 Madison Ave  Condo for sale  899500   \n",
      "\n",
      "   BEDS  BATH  PROPERTYSQFT                  ADDRESS                    STATE  \\\n",
      "0     2   2.0   1400.000000     2 E 55th St Unit 803       New York, NY 10022   \n",
      "1     4   2.0   2015.000000         620 Sinclair Ave  Staten Island, NY 10312   \n",
      "2     3   1.0    445.000000  2 E 55th St Unit 908W33      Manhattan, NY 10022   \n",
      "3     5   2.0   4004.000000              584 Park Pl       Brooklyn, NY 11238   \n",
      "4     2   2.0   2184.207862   157 W 126th St Unit 1B       New York, NY 10027   \n",
      "\n",
      "                                 MAIN_ADDRESS ADMINISTRATIVE_AREA_LEVEL_2  \\\n",
      "0      2 E 55th St Unit 803New York, NY 10022             New York County   \n",
      "1     620 Sinclair AveStaten Island, NY 10312               United States   \n",
      "2  2 E 55th St Unit 908W33Manhattan, NY 10022               United States   \n",
      "3               584 Park PlBrooklyn, NY 11238               United States   \n",
      "4    157 W 126th St Unit 1BNew York, NY 10027                    New York   \n",
      "\n",
      "          LOCALITY      SUBLOCALITY       STREET_NAME         LONG_NAME  \\\n",
      "0         New York        Manhattan  East 55th Street   Regis Residence   \n",
      "1         New York  Richmond County     Staten Island   Sinclair Avenue   \n",
      "2         New York  New York County          New York  East 55th Street   \n",
      "3         New York     Kings County          Brooklyn        Park Place   \n",
      "4  New York County         New York         Manhattan               157   \n",
      "\n",
      "                                   FORMATTED_ADDRESS   LATITUDE  LONGITUDE  \n",
      "0  Regis Residence, 2 E 55th St #803, New York, N...  40.761255 -73.974483  \n",
      "1     620 Sinclair Ave, Staten Island, NY 10312, USA  40.541805 -74.196109  \n",
      "2               2 E 55th St, New York, NY 10022, USA  40.761398 -73.974613  \n",
      "3               584 Park Pl, Brooklyn, NY 11238, USA  40.674363 -73.958725  \n",
      "4        157 W 126th St #1b, New York, NY 10027, USA  40.809448 -73.946777  \n",
      "\n",
      "Preparing features...\n",
      "\n",
      "Training LightGBM model...\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m lgb_train \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_train, y_train)\n\u001b[0;32m     68\u001b[0m lgb_test \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_test, y_test, reference\u001b[38;5;241m=\u001b[39mlgb_train)\n\u001b[1;32m---> 69\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(lgb_params, lgb_train, valid_sets\u001b[38;5;241m=\u001b[39m[lgb_test])\n\u001b[0;32m     71\u001b[0m y_pred_lgb \u001b[38;5;241m=\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     72\u001b[0m rmse_lgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_test, y_pred_lgb))\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m Booster(params\u001b[38;5;241m=\u001b[39mparams, train_set\u001b[38;5;241m=\u001b[39mtrain_set)\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:3660\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n\u001b[0;32m   3659\u001b[0m params_str \u001b[38;5;241m=\u001b[39m _param_dict_to_str(params)\n\u001b[1;32m-> 3660\u001b[0m _safe_call(\n\u001b[0;32m   3661\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterCreate(\n\u001b[0;32m   3662\u001b[0m         train_set\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   3663\u001b[0m         _c_str(params_str),\n\u001b[0;32m   3664\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle),\n\u001b[0;32m   3665\u001b[0m     )\n\u001b[0;32m   3666\u001b[0m )\n\u001b[0;32m   3667\u001b[0m \u001b[38;5;66;03m# save reference to data\u001b[39;00m\n\u001b[0;32m   3668\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set \u001b[38;5;241m=\u001b[39m train_set\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:313\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Try to find the CSV file in the current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(current_dir)\n",
    "csv_files = [f for f in files if f.endswith('.csv')]\n",
    "print(f\"CSV files found: {csv_files}\")\n",
    "\n",
    "# Try to load the NYC_House_Dataset-Cleaned.csv file\n",
    "if 'NYC_House_Dataset-Cleaned.csv' in csv_files:\n",
    "    file_to_load = 'NYC_House_Dataset-Cleaned.csv'\n",
    "elif len(csv_files) > 0:\n",
    "    # If specific file not found, use the first CSV file\n",
    "    file_to_load = csv_files[0]\n",
    "    print(f\"Using {file_to_load} instead of NYC_House_Dataset-Cleaned.csv\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV files found in the current directory\")\n",
    "\n",
    "# Load the dataset\n",
    "print(f\"Loading {file_to_load}...\")\n",
    "dataframe_cleaned = pd.read_csv(file_to_load)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Shape: {dataframe_cleaned.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "print(dataframe_cleaned.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(dataframe_cleaned.head())\n",
    "\n",
    "# Step 2: Feature engineering and model training\n",
    "# Encode categorical variables (SUBLOCALITY) as dummy variables\n",
    "print(\"\\nPreparing features...\")\n",
    "X = pd.get_dummies(dataframe_cleaned[['PROPERTYSQFT', 'BEDS', 'BATH', 'SUBLOCALITY']], drop_first=True)\n",
    "y = dataframe_cleaned['PRICE']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. LightGBM with GPU support (if enabled, otherwise CPU)\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'device': 'gpu',  # Use GPU (RTX 4060) if available, otherwise CPU\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_test])\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\n",
    "print(f\"LightGBM RMSE: {rmse_lgb:.2f}\")\n",
    "\n",
    "# 2. XGBoost with GPU support\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',  # Use GPU if available\n",
    "    'gpu_id': 0,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 6,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "print(f\"XGBoost RMSE: {rmse_xgb:.2f}\")\n",
    "\n",
    "# 3. CatBoost (CPU, as GPU support is less straightforward)\n",
    "print(\"\\nTraining CatBoost model...\")\n",
    "cb_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# CatBoost can handle categorical features directly, so we don't need dummy variables\n",
    "X_cat = dataframe_cleaned[['PROPERTYSQFT', 'BEDS', 'BATH', 'SUBLOCALITY']]\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y, test_size=0.2, random_state=42)\n",
    "\n",
    "cb_model = cb.CatBoostRegressor(**cb_params)\n",
    "cb_model.fit(X_train_cat, y_train_cat, eval_set=(X_test_cat, y_test_cat), cat_features=['SUBLOCALITY'])\n",
    "\n",
    "y_pred_cb = cb_model.predict(X_test_cat)\n",
    "rmse_cb = np.sqrt(mean_squared_error(y_test_cat, y_pred_cb))\n",
    "print(f\"CatBoost RMSE: {rmse_cb:.2f}\")\n",
    "\n",
    "# Choose the best model (lowest RMSE)\n",
    "print(\"\\nComparing models...\")\n",
    "models = [\n",
    "    ('LightGBM', lgb_model, rmse_lgb),\n",
    "    ('XGBoost', xgb_model, rmse_xgb),\n",
    "    ('CatBoost', cb_model, rmse_cb)\n",
    "]\n",
    "best_model = min(models, key=lambda x: x[2])\n",
    "\n",
    "print(f\"Best Model: {best_model[0]} with RMSE: {best_model[2]:.2f}\")\n",
    "\n",
    "# Use the best model to predict prices for the entire dataset\n",
    "print(\"\\nGenerating predictions with best model...\")\n",
    "if best_model[0] == 'LightGBM':\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = lgb_model.predict(X)\n",
    "elif best_model[0] == 'XGBoost':\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = xgb_model.predict(X)\n",
    "else:\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = cb_model.predict(X_cat)\n",
    "\n",
    "# Calculate price difference\n",
    "dataframe_cleaned['PRICE_DIFFERENCE'] = dataframe_cleaned['PREDICTED_PRICE'] - dataframe_cleaned['PRICE']\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPrediction Summary:\")\n",
    "print(f\"Average actual price: ${dataframe_cleaned['PRICE'].mean():.2f}\")\n",
    "print(f\"Average predicted price: ${dataframe_cleaned['PREDICTED_PRICE'].mean():.2f}\")\n",
    "print(f\"Average price difference: ${dataframe_cleaned['PRICE_DIFFERENCE'].abs().mean():.2f}\")\n",
    "print(f\"Percentage error: {(dataframe_cleaned['PRICE_DIFFERENCE'].abs().mean() / dataframe_cleaned['PRICE'].mean() * 100):.2f}%\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(dataframe_cleaned[['PRICE', 'PREDICTED_PRICE', 'PRICE_DIFFERENCE']].head(10))\n",
    "\n",
    "# Save results if needed\n",
    "# dataframe_cleaned.to_csv('nyc_housing_predictions.csv', index=False)\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Dataset ---\n",
      "Current working directory: c:\\Users\\suhas\\OneDrive\\Desktop\\PROPGROWTHX  AI PROJECT TRIAL\n",
      "CSV files found: ['NY-House-Dataset-Modified.csv', 'NY-House-Dataset.csv', 'NYC_House_Dataset-Cleaned.csv']\n",
      "Loading NYC_House_Dataset-Cleaned.csv...\n",
      "\n",
      "Dataset Info:\n",
      "Shape: (4552, 17)\n",
      "\n",
      "Columns:\n",
      "['BROKERTITLE', 'TYPE', 'PRICE', 'BEDS', 'BATH', 'PROPERTYSQFT', 'ADDRESS', 'STATE', 'MAIN_ADDRESS', 'ADMINISTRATIVE_AREA_LEVEL_2', 'LOCALITY', 'SUBLOCALITY', 'STREET_NAME', 'LONG_NAME', 'FORMATTED_ADDRESS', 'LATITUDE', 'LONGITUDE']\n",
      "\n",
      "First few rows:\n",
      "                                     BROKERTITLE            TYPE   PRICE  \\\n",
      "0    Brokered by Douglas Elliman  -111 Fifth Ave  Condo for sale  315000   \n",
      "1                         Brokered by Sowae Corp  House for sale  260000   \n",
      "2                            Brokered by COMPASS  Condo for sale   69000   \n",
      "3                         Brokered by Sowae Corp  House for sale  690000   \n",
      "4  Brokered by Douglas Elliman - 575 Madison Ave  Condo for sale  899500   \n",
      "\n",
      "   BEDS  BATH  PROPERTYSQFT                  ADDRESS                    STATE  \\\n",
      "0     2   2.0   1400.000000     2 E 55th St Unit 803       New York, NY 10022   \n",
      "1     4   2.0   2015.000000         620 Sinclair Ave  Staten Island, NY 10312   \n",
      "2     3   1.0    445.000000  2 E 55th St Unit 908W33      Manhattan, NY 10022   \n",
      "3     5   2.0   4004.000000              584 Park Pl       Brooklyn, NY 11238   \n",
      "4     2   2.0   2184.207862   157 W 126th St Unit 1B       New York, NY 10027   \n",
      "\n",
      "                                 MAIN_ADDRESS ADMINISTRATIVE_AREA_LEVEL_2  \\\n",
      "0      2 E 55th St Unit 803New York, NY 10022             New York County   \n",
      "1     620 Sinclair AveStaten Island, NY 10312               United States   \n",
      "2  2 E 55th St Unit 908W33Manhattan, NY 10022               United States   \n",
      "3               584 Park PlBrooklyn, NY 11238               United States   \n",
      "4    157 W 126th St Unit 1BNew York, NY 10027                    New York   \n",
      "\n",
      "          LOCALITY      SUBLOCALITY       STREET_NAME         LONG_NAME  \\\n",
      "0         New York        Manhattan  East 55th Street   Regis Residence   \n",
      "1         New York  Richmond County     Staten Island   Sinclair Avenue   \n",
      "2         New York  New York County          New York  East 55th Street   \n",
      "3         New York     Kings County          Brooklyn        Park Place   \n",
      "4  New York County         New York         Manhattan               157   \n",
      "\n",
      "                                   FORMATTED_ADDRESS   LATITUDE  LONGITUDE  \n",
      "0  Regis Residence, 2 E 55th St #803, New York, N...  40.761255 -73.974483  \n",
      "1     620 Sinclair Ave, Staten Island, NY 10312, USA  40.541805 -74.196109  \n",
      "2               2 E 55th St, New York, NY 10022, USA  40.761398 -73.974613  \n",
      "3               584 Park Pl, Brooklyn, NY 11238, USA  40.674363 -73.958725  \n",
      "4        157 W 126th St #1b, New York, NY 10027, USA  40.809448 -73.946777  \n",
      "\n",
      "--- Step 2: Feature Engineering ---\n",
      "\n",
      "Adding proximity feature (Distance to Manhattan)...\n",
      "Distance to Manhattan (km) calculated and added as a feature.\n",
      "\n",
      "Preparing features for model training...\n",
      "\n",
      "--- Step 3: Model Training ---\n",
      "\n",
      "Training LightGBM model...\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m lgb_train \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_train, y_train)\n\u001b[0;32m    111\u001b[0m lgb_test \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_test, y_test, reference\u001b[38;5;241m=\u001b[39mlgb_train)\n\u001b[1;32m--> 112\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(lgb_params, lgb_train, valid_sets\u001b[38;5;241m=\u001b[39m[lgb_test])\n\u001b[0;32m    114\u001b[0m y_pred_lgb \u001b[38;5;241m=\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    115\u001b[0m rmse_lgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_test, y_pred_lgb))\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m Booster(params\u001b[38;5;241m=\u001b[39mparams, train_set\u001b[38;5;241m=\u001b[39mtrain_set)\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:3660\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n\u001b[0;32m   3659\u001b[0m params_str \u001b[38;5;241m=\u001b[39m _param_dict_to_str(params)\n\u001b[1;32m-> 3660\u001b[0m _safe_call(\n\u001b[0;32m   3661\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterCreate(\n\u001b[0;32m   3662\u001b[0m         train_set\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   3663\u001b[0m         _c_str(params_str),\n\u001b[0;32m   3664\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle),\n\u001b[0;32m   3665\u001b[0m     )\n\u001b[0;32m   3666\u001b[0m )\n\u001b[0;32m   3667\u001b[0m \u001b[38;5;66;03m# save reference to data\u001b[39;00m\n\u001b[0;32m   3668\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set \u001b[38;5;241m=\u001b[39m train_set\n",
      "File \u001b[1;32mc:\\Users\\suhas\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:313\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from geopy.distance import geodesic  # For calculating distances\n",
    "\n",
    "# --- Step 1: Load the dataset ---\n",
    "print(\"--- Step 1: Loading Dataset ---\")\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "files = os.listdir(current_dir)\n",
    "csv_files = [f for f in files if f.endswith('.csv')]\n",
    "print(f\"CSV files found: {csv_files}\")\n",
    "\n",
    "file_to_load = None\n",
    "if 'NYC_House_Dataset-Cleaned.csv' in csv_files:\n",
    "    file_to_load = 'NYC_House_Dataset-Cleaned.csv'\n",
    "elif csv_files:\n",
    "    file_to_load = csv_files[0]\n",
    "    print(f\"Using {file_to_load} instead of NYC_House_Dataset-Cleaned.csv\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV files found in the current directory\")\n",
    "\n",
    "print(f\"Loading {file_to_load}...\")\n",
    "try:\n",
    "    dataframe_cleaned = pd.read_csv(file_to_load)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Shape: {dataframe_cleaned.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "print(dataframe_cleaned.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(dataframe_cleaned.head())\n",
    "\n",
    "# --- Step 2: Feature Engineering ---\n",
    "print(\"\\n--- Step 2: Feature Engineering ---\")\n",
    "\n",
    "# 2.1: Proximity Feature (Distance to City Center)\n",
    "print(\"\\nAdding proximity feature (Distance to Manhattan)...\")\n",
    "# Define the coordinates of a central point in Manhattan (you can adjust this)\n",
    "manhattan_center = (40.7590, -73.9845)  # Approximate center of Midtown\n",
    "\n",
    "def calculate_distance_to_center(row, center_coords):\n",
    "    if pd.notna(row['LATITUDE']) and pd.notna(row['LONGITUDE']):\n",
    "        property_coords = (row['LATITUDE'], row['LONGITUDE'])\n",
    "        return geodesic(property_coords, center_coords).km\n",
    "    return np.nan\n",
    "\n",
    "if 'LATITUDE' in dataframe_cleaned.columns and 'LONGITUDE' in dataframe_cleaned.columns:\n",
    "    dataframe_cleaned['distance_to_manhattan_km'] = dataframe_cleaned.apply(\n",
    "        lambda row: calculate_distance_to_center(row, manhattan_center), axis=1\n",
    "    )\n",
    "    print(\"Distance to Manhattan (km) calculated and added as a feature.\")\n",
    "else:\n",
    "    print(\"Latitude or Longitude columns not found. Cannot calculate distance to Manhattan.\")\n",
    "    dataframe_cleaned['distance_to_manhattan_km'] = np.nan  # Add even if calculation fails\n",
    "\n",
    "# 2.2: Prepare features for model training\n",
    "print(\"\\nPreparing features for model training...\")\n",
    "categorical_features = ['SUBLOCALITY']\n",
    "numerical_features = ['PROPERTYSQFT', 'BEDS', 'BATH', 'distance_to_manhattan_km']\n",
    "\n",
    "# Handle potential missing values in numerical features (impute with median for now)\n",
    "for col in numerical_features:\n",
    "    if col in dataframe_cleaned.columns:\n",
    "        dataframe_cleaned[col] = dataframe_cleaned[col].fillna(dataframe_cleaned[col].median())\n",
    "    else:\n",
    "        print(f\"Warning: Numerical feature '{col}' not found in the dataset.\")\n",
    "\n",
    "# Ensure all expected columns are present before creating dummy variables\n",
    "expected_cols = numerical_features + categorical_features\n",
    "missing_cols = [col for col in expected_cols if col not in dataframe_cleaned.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing expected columns in DataFrame: {missing_cols}\")\n",
    "\n",
    "X = pd.get_dummies(dataframe_cleaned[numerical_features + categorical_features], drop_first=True)\n",
    "y = dataframe_cleaned['PRICE']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Step 3: Model Training ---\n",
    "print(\"\\n--- Step 3: Model Training ---\")\n",
    "\n",
    "# 3.1: LightGBM with GPU support\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'device': 'gpu',  # Use GPU if available\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_test])\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\n",
    "print(f\"LightGBM RMSE: {rmse_lgb:.2f}\")\n",
    "\n",
    "# 3.2: XGBoost with GPU support\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',  # Use GPU if available\n",
    "    'gpu_id': 0,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 6,\n",
    "    'eval_metric': 'rmse',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "print(f\"XGBoost RMSE: {rmse_xgb:.2f}\")\n",
    "\n",
    "# 3.3: CatBoost\n",
    "print(\"\\nTraining CatBoost model...\")\n",
    "cb_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 0,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "X_cat_cols = numerical_features + categorical_features\n",
    "X_cat = dataframe_cleaned[X_cat_cols]\n",
    "y_cat = dataframe_cleaned['PRICE'] # Use the same y as the other models\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "cb_model = cb.CatBoostRegressor(**cb_params)\n",
    "cb_model.fit(X_train_cat, y_train_cat, eval_set=(X_test_cat, y_test_cat), cat_features=categorical_features)\n",
    "\n",
    "y_pred_cb = cb_model.predict(X_test_cat)\n",
    "rmse_cb = np.sqrt(mean_squared_error(y_test_cat, y_pred_cb))\n",
    "print(f\"CatBoost RMSE: {rmse_cb:.2f}\")\n",
    "\n",
    "# --- Step 4: Model Comparison and Selection ---\n",
    "print(\"\\n--- Step 4: Model Comparison and Selection ---\")\n",
    "models = [\n",
    "    ('LightGBM', lgb_model, rmse_lgb),\n",
    "    ('XGBoost', xgb_model, rmse_xgb),\n",
    "    ('CatBoost', cb_model, rmse_cb)\n",
    "]\n",
    "best_model = min(models, key=lambda x: x[2])\n",
    "\n",
    "print(f\"Best Model: {best_model[0]} with RMSE: {best_model[2]:.2f}\")\n",
    "\n",
    "# --- Step 5: Generating Predictions and Analyzing Results ---\n",
    "print(\"\\n--- Step 5: Generating Predictions and Analyzing Results ---\")\n",
    "\n",
    "# 5.1: Predict prices for the entire dataset with the best model\n",
    "print(\"\\nGenerating predictions with the best model...\")\n",
    "if best_model[0] == 'LightGBM':\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = best_model[1].predict(X)\n",
    "elif best_model[0] == 'XGBoost':\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = best_model[1].predict(X)\n",
    "else:\n",
    "    X_cat_full = dataframe_cleaned[numerical_features + categorical_features]\n",
    "    dataframe_cleaned['PREDICTED_PRICE'] = best_model[1].predict(X_cat_full)\n",
    "\n",
    "# 5.2: Calculate price difference\n",
    "dataframe_cleaned['PRICE_DIFFERENCE'] = dataframe_cleaned['PREDICTED_PRICE'] - dataframe_cleaned['PRICE']\n",
    "\n",
    "# 5.3: Display overall prediction summary\n",
    "print(\"\\nOverall Prediction Summary:\")\n",
    "print(f\"Average actual price: ${dataframe_cleaned['PRICE'].mean():.2f}\")\n",
    "print(f\"Average predicted price: ${dataframe_cleaned['PREDICTED_PRICE'].mean():.2f}\")\n",
    "print(f\"Average price difference: ${dataframe_cleaned['PRICE_DIFFERENCE'].abs().mean():.2f}\")\n",
    "print(f\"Percentage error: {(dataframe_cleaned['PRICE_DIFFERENCE'].abs().mean() / dataframe_cleaned['PRICE'].mean() * 100):.2f}%\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(dataframe_cleaned[['PRICE', 'PREDICTED_PRICE', 'PRICE_DIFFERENCE', 'SUBLOCALITY', 'distance_to_manhattan_km']].head(10))\n",
    "\n",
    "# 5.4: Analyze performance by Sublocality (as requested)\n",
    "print(\"\\n--- Analyzing Performance by Sublocality ---\")\n",
    "if best_model[0] == 'CatBoost':\n",
    "    y_pred_test = best_model[1].predict(X_test_cat)\n",
    "    results_df = pd.DataFrame({'Actual Price': y_test_cat, 'Predicted Price': y_pred_test, 'SUBLOCALITY': X_test_cat['SUBLOCALITY']})\n",
    "else:\n",
    "    results_df = pd.DataFrame({'Actual Price': y_test, 'Predicted Price': y_pred_lgb if best_model[0] == 'LightGBM' else y_pred_xgb, 'SUBLOCALITY': dataframe_cleaned.loc[X_test.index, 'SUBLOCALITY'].values}) # Use .values to avoid potential index misalignment\n",
    "\n",
    "results_df['Price Difference'] = results_df['Predicted Price'] - results_df['Actual Price']\n",
    "sublocality_analysis = results_df.groupby('SUBLOCALITY').agg(\n",
    "    count=('Actual Price', 'count'),\n",
    "    mean_actual_price=('Actual Price', 'mean'),\n",
    "    mean_predicted_price=('Predicted Price', 'mean'),\n",
    "    rmse=('Price Difference', lambda x: np.sqrt(np.mean(x**2))),\n",
    "    mean_error=('Price Difference', 'mean')\n",
    ").sort_values(by='mean_actual_price', ascending=False)\n",
    "\n",
    "print(\"\\nModel Performance by Sublocality (sorted by average actual price):\")\n",
    "print(sublocality_analysis)\n",
    "\n",
    "# 5.5: Feature Importance (for tree-based models)\n",
    "print(\"\\n--- Feature Importance ---\")\n",
    "if best_model[0] == 'LightGBM':\n",
    "    feature_importance = pd.Series(lgb_model.feature_importance(), index=X_train.columns)\n",
    "    print(\"\\nLightGBM Feature Importance:\")\n",
    "    print(feature_importance.sort_values(ascending=False))\n",
    "elif best_model[0] == 'XGBoost':\n",
    "    feature_importance = pd.Series(xgb_model.feature_importances_, index=X_train.columns)\n",
    "    print(\"\\nXGBoost Feature Importance:\")\n",
    "    print(feature_importance.sort_values(ascending=False))\n",
    "elif best_model[0] == 'CatBoost':\n",
    "    feature_importance = pd.Series(cb_model.get_feature_importance(), index=X_train_cat.columns)\n",
    "    print(\"\\nCatBoost Feature Importance:\")\n",
    "    print(feature_importance.sort_values(ascending=False))\n",
    "\n",
    "# Save results if needed\n",
    "# dataframe_cleaned.to_csv('nyc_housing_predictions_improved.csv', index=False)\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: lightgbm 4.6.0\n",
      "Uninstalling lightgbm-4.6.0:\n",
      "  Successfully uninstalled lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall lightgbm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\suhas\\anaconda3\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  c:\\Users\\suhas\\anaconda3\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  c:\\Users\\suhas\\anaconda3\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  c:\\Users\\suhas\\anaconda3\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  c:\\Users\\suhas\\anaconda3\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --install-option\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm --install-option=--gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmake\n",
      "  Downloading cmake-4.0.0-py3-none-win_amd64.whl.metadata (6.3 kB)\n",
      "Downloading cmake-4.0.0-py3-none-win_amd64.whl (36.7 MB)\n",
      "   ---------------------------------------- 0.0/36.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 6.0/36.7 MB 30.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 10.5/36.7 MB 25.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 13.6/36.7 MB 22.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 18.9/36.7 MB 22.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 23.1/36.7 MB 22.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 27.0/36.7 MB 22.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 31.2/36.7 MB 21.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 35.4/36.7 MB 21.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 36.7/36.7 MB 21.0 MB/s eta 0:00:00\n",
      "Installing collected packages: cmake\n",
      "Successfully installed cmake-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cmake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodule path 'external_libs/compute': checked out '36350b7de849300bd3d72a05d8bf890ca405a014'\n",
      "Submodule path 'external_libs/eigen': checked out '3147391d946bb4b6c68edd901f2add6ac1f31f8c'\n",
      "Submodule path 'external_libs/fast_double_parser': checked out '252029ddac664370bdda3f0761675785d92a1573'\n",
      "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp': checked out 'd936052d32a5b7ca08b0199a6724724aea432309'\n",
      "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion': checked out 'f4cb2384efa55dee0e6652f8674b05763441ab09'\n",
      "Submodule path 'external_libs/fmt': checked out '8303d140a1a11f19b982a9f664bbe59a1ccda3f4'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'LightGBM'...\n",
      "Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'external_libs/compute'\n",
      "Submodule 'eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'external_libs/eigen'\n",
      "Submodule 'external_libs/fast_double_parser' (https://github.com/lemire/fast_double_parser.git) registered for path 'external_libs/fast_double_parser'\n",
      "Submodule 'external_libs/fmt' (https://github.com/fmtlib/fmt.git) registered for path 'external_libs/fmt'\n",
      "Cloning into 'C:/Users/suhas/OneDrive/Desktop/PROPGROWTHX  AI PROJECT TRIAL/LightGBM/external_libs/compute'...\n",
      "Cloning into 'C:/Users/suhas/OneDrive/Desktop/PROPGROWTHX  AI PROJECT TRIAL/LightGBM/external_libs/eigen'...\n",
      "Cloning into 'C:/Users/suhas/OneDrive/Desktop/PROPGROWTHX  AI PROJECT TRIAL/LightGBM/external_libs/fast_double_parser'...\n",
      "Cloning into 'C:/Users/suhas/OneDrive/Desktop/PROPGROWTHX  AI PROJECT TRIAL/LightGBM/external_libs/fmt'...\n",
      "Submodule 'benchmark/dependencies/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'\n",
      "Submodule 'benchmark/dependencies/double-conversion' (https://github.com/google/double-conversion.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'\n",
      "Cloning into 'C:/Users/suhas/OneDrive/Desktop/PROPGROWTHX  AI PROJECT TRIAL/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'...\n",
      "Cloning into 'C:/Users/suhas/OneDrive/Desktop/PROPGROWTHX  AI PROJECT TRIAL/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'...\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/microsoft/LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'lightgbm' has no attribute 'get_device_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(lgb\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(lgb\u001b[38;5;241m.\u001b[39mget_device_name())\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'lightgbm' has no attribute 'get_device_name'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "print(lgb.get_device_name())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ME TRYING (ABDUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_regression\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Print LightGBM version\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "\n",
    "# Generate a small dataset for testing\n",
    "X, y = make_regression(n_samples=10000, n_features=20, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define parameters with GPU support\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'device': 'gpu',  # Enable GPU\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'max_bin': 63,  # Optimize for GPU\n",
    "    'gpu_use_dp': False,  # Use single precision\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 100,\n",
    "    'verbose': 1,  # Enable verbose logging to see GPU usage\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training with GPU...\")\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data], callbacks=[lgb.early_stopping(stopping_rounds=10)])\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(((y_pred - y_test) ** 2).mean())\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 25.3 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping lightgbm as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall lightgbm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(lightgbm\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(lightgbm\u001b[38;5;241m.\u001b[39m__path__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "print(lightgbm.__version__)\n",
    "print(lightgbm.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: conda\n",
      "Version: 24.11.3\n",
      "Summary: OS-agnostic, system-level binary package manager.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \"Anaconda, Inc.\" <conda@continuum.io>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2012, Anaconda, Inc.\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "    * Redistributions of source code must retain the above copyright\n",
      "      notice, this list of conditions and the following disclaimer.\n",
      "    * Redistributions in binary form must reproduce the above copyright\n",
      "      notice, this list of conditions and the following disclaimer in the\n",
      "      documentation and/or other materials provided with the distribution.\n",
      "    * Neither the name of the copyright holder nor the names of its\n",
      "      contributors may be used to endorse or promote products\n",
      "      derived from this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
      "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
      "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY\n",
      "DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
      "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
      "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
      "ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
      "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
      "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "---\n",
      "\n",
      "Conda incorporates the following libraries into its distributed code:\n",
      "\n",
      "* auxlib, licensed as ISC\n",
      "* frozendict, license as LGPL-3.0\n",
      "Location: C:\\Users\\suhas\\anaconda3\\Lib\\site-packages\n",
      "Requires: archspec, boltons, charset-normalizer, conda-libmamba-solver, conda-package-handling, distro, frozendict, jsonpatch, menuinst, packaging, platformdirs, pluggy, pycosat, requests, ruamel-yaml, setuptools, tqdm, truststore, zstandard\n",
      "Required-by: anaconda-anon-usage, conda-build, conda-libmamba-solver, conda-token, conda_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: #, `!conda, if, lightgbm, lightgbm`, list, or, using\n"
     ]
    }
   ],
   "source": [
    "!pip show lightgbm  # or `!conda list lightgbm` if using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pip\n",
      "Version: 24.2\n",
      "Summary: The PyPA recommended tool for installing Python packages.\n",
      "Home-page: https://pip.pypa.io/\n",
      "Author: \n",
      "Author-email: The pip developers <distutils-sig@python.org>\n",
      "License: MIT\n",
      "Location: C:\\Users\\suhas\\anaconda3\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: #, For, lightgbm\n",
      "usage: conda-script.py [-h] [-v] [--no-plugins] [-V] COMMAND ...\n",
      "conda-script.py: error: unrecognized arguments: # For conda\n"
     ]
    }
   ],
   "source": [
    "!pip show lightgbm         # For pip\n",
    "!conda list lightgbm       # For conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\suhas\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Using cached lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: lightgbm\n",
      "Version: 4.6.0\n",
      "Summary: LightGBM Python-package\n",
      "Home-page: https://github.com/microsoft/LightGBM\n",
      "Author: \n",
      "Author-email: \n",
      "License: The MIT License (MIT)\n",
      "\n",
      "Copyright (c) Microsoft Corporation\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "Location: C:\\Users\\suhas\\anaconda3\\Lib\\site-packages\n",
      "Requires: numpy, scipy\n",
      "Required-by: \n",
      "---\n",
      "Name: lightgbm\n",
      "Version: 4.6.0\n",
      "Summary: LightGBM Python-package\n",
      "Home-page: https://github.com/microsoft/LightGBM\n",
      "Author: \n",
      "Author-email: \n",
      "License: The MIT License (MIT)\n",
      "\n",
      "Copyright (c) Microsoft Corporation\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "Location: C:\\Users\\suhas\\anaconda3\\Lib\\site-packages\n",
      "Requires: numpy, scipy\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: !conda, #, list, or\n"
     ]
    }
   ],
   "source": [
    "!pip show lightgbm  # or !conda list lightgbm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
